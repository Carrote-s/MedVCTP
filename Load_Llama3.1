!pip install transformers

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- Load LLaMA
llm_model_name = "meta-llama/Llama-3.1-8B-Instruct"  # or your variant
llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
llm_model = AutoModelForCausalLM.from_pretrained(
    llm_model_name,
    dtype=torch.bfloat16  # optional, if GPU supports
)
llm_model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
llm_model.to(device)

# --- Callable function
def run_llama(messages, max_new_tokens=300):
    inputs = llm_tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt",
    ).to(device)
    
    outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=llm_tokenizer.eos_token_id)
    
    return llm_tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True
    )

