Code for our paper *MedVCTP: Improving Accuracy and Explainability in Medical Visual Reasoning* 

# **Overall Framework**

<img width="719" height="480" alt="4cf577feaa13c90d0bcfff677a8b85a722377e1a" src="https://github.com/user-attachments/assets/5b6fe8f8-bdfa-4a56-9b3f-d4ae650de71c" />

# **Preprocess dataset**

* Voxel51 SLAKE dataset
* To download the dataset, run the **Load_SLAKE.py** script.

# **Prepare Experiments**

* Once the SLAKE dataset is fully loaded in, the goal is to successfully load in the isolated close-ended SLAKE images with questions, assuming that bounding boxes are provided for all images.
* We use MedGemma to generate captioning, more specifically the 4b instruction-tuned model. We provide the prompts utilized for MedGemma along with the actual code itself in the repository. This can be helpful for precomputing the visual features. 
* MedGemma is open source, and captions can be generated by prompt tuning. We also still provide the global and regional captions, along with other datasets specified in the repository. 
* The main step after loading the SLAKE dataset is to load LLama 3.1 and BioMedCLIP. It is recommended to load in LLama 3.1 first, and then load BioMedCLIP later. You can use the respective code-files provided. Keep in mind that if you are running the ablation, only LLama 3.1 is needed. 
* Once the models are fully loaded, use the function files depending on the experiment you want to run. Make sure that the functions utilize the few shot examples provided in the repository files. We have provided the specific locations on where the FewShotExamples will be loaded, named examples5 in the functions.
* When the functions and the models are fully loaded in and functional, it is time to iterate through the dataset and cross-check outputs to attain accuracy. We use the code provided, starting with run and then ending with the respective desired experiment.
* The run functions may need to be slightly modified depending on the format of the SLAKE files you input, but to mitigate this, we provide the files ourselves in the repository.
* After all functions are loaded, the code should successfully run. If accuracy seems to be abnormally low, check that you have utilized the normalize function in the function files. This function is responsible for removing punctuation and capital letters when checking answers for accuracy. Also make sure that the right files are used, and that the code aligns with such files.
* For extra support, we have provided the entire MedVCTP code to help users figure out any gaps between our implementation and their implementation. It can help with any questions you have about implementation.
* A quick heads up: In the future, biomedclip and llama 3.1 may have compatibility issues regarding the transformers module. At the time we ran this code and extracted results, the MedVCTP.ipynb worked completely fine, we simply downgraded the transformer packages after loading in llama 3.1 for loading in biomedclip, as biomedclip required transformers version 4.35.2 whilst llama 3.1 8b required transformers 4.43 and onwards. The code was run in the beginning of September 2025, so we were using an appropriate transformer module version then. If incompatibility is persistent when trying downgrading, create seperate environments for both models, and potentially could use persistent processes by putting both models in its own long lived environment to help runtime, just a suggestion. There are other options available. 

# **Run Experiments**

* Run_MedVCTP.py for MedVCTP
* Run_Ablation.py for Ablation study
* MedVCTP.py or MedVCTP.ipynb for all main code, doesn't include medgemma configurations and computations


# ** Voxel51 SLAKE dataset **
* We additionally run our pipeline on VQA - RAD, which doesn't provide any bounding boxes. Because kaggle's version is the only version that has seperate image IDs to index images easier, we download it into a google drive, and mount google drive onto jupyter notebook using an API.
* We load all images directly into the notebook for usage, and generate and properly index a caption for every single unique image for the close ended questions which is 294 out of 315 images. The specific prompting details and how we configured and used MedGemma for VQARAD is located in VQARAD.ipynb. 
