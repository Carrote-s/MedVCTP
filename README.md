Code for our paper *MedVCTP: Improving Accuracy and Explainability in Medical Visual Reasoning* 

# **Overall Framework**

<img width="719" height="480" alt="4cf577feaa13c90d0bcfff677a8b85a722377e1a" src="https://github.com/user-attachments/assets/5b6fe8f8-bdfa-4a56-9b3f-d4ae650de71c" />

# **Preprocess dataset**

* Voxel51 SLAKE dataset
* To download the dataset, run the **Load_SLAKE.py** script.

# **Prepare Experiments**

* Once the SLAKE dataset is fully loaded in, the goal is to successfully load in the isolated close-ended SLAKE images with questions, assuming that bounding boxes are provided for all images.
* We use MedGemma to generate captioning, more specifically the 4b instruction-tuned model. We don't keep the code in this github because the purpose of this repository is to demonstrate the main VCTP pipeline.
* MedGemma is still open source, and captions can be generated by prompt tuning.
* The main step after loading the SLAKE dataset is to load LLama 3.1 and BioMedCLIP. It is recommended to load in LLama 3.1 first, and then load BioMedCLIP later. You can use the respective code-files provided. Keep in mind that if you are running the ablation, only LLama 3.1 is needed. 
* Once the models are fully loaded, use the function files depending on the experiment you want to run. Make sure that the functions utilize the few shot examples provided in the repository files. 

# **Run Experiments**

* Run_MedVCTP.py for MedVCTP
* Run_Ablation.py for Ablation study
