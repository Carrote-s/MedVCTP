{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aG_wkbioTOt"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()  # paste your HF to|"
      ],
      "metadata": {
        "id": "11opFH6doVtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load huggingface\n",
        "# installations\n",
        "\n",
        "!pip install -U fiftyone\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "id": "ijqY7V0OoXzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load voxel 51 SLAKE\n",
        "import fiftyone as fo\n",
        "\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "\n",
        "  # Load the dataset\n",
        "\n",
        "  # Note: other available arguments include 'max_samples', etc\n",
        "\n",
        "dataset = load_from_hub(\"Voxel51/SLAKE\")"
      ],
      "metadata": {
        "id": "6JPHpd8ioX-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "VeeB1bGFoYQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# --- Load LLaMA\n",
        "llm_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  # or your variant\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_model_name,\n",
        "    dtype=torch.bfloat16  # optional, if GPU supports\n",
        ")\n",
        "llm_model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "llm_model.to(device)"
      ],
      "metadata": {
        "id": "AbMCT3jIoYcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Callable function\n",
        "def run_llama(messages, max_new_tokens=300):\n",
        "    inputs = llm_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = llm_model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=llm_tokenizer.eos_token_id)\n",
        "\n",
        "    return llm_tokenizer.decode(\n",
        "        outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True\n",
        "    )"
      ],
      "metadata": {
        "id": "ZW6vPUrYoYrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install open_clip_torch==2.23.0 transformers==4.35.2 matplotlib"
      ],
      "metadata": {
        "id": "fFKV38hfoY8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from open_clip import create_model_from_pretrained, get_tokenizer # works on open-clip-torch>=2.23.0, timm>=0.9.8\n",
        "\n",
        "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
        "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
      ],
      "metadata": {
        "id": "NfPEV2XyoZOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from open_clip import create_model_and_transforms, get_tokenizer\n",
        "from PIL import Image\n",
        "\n",
        "class BiomedCLIPConfirm:\n",
        "    def __init__(self,\n",
        "                 model_name=\"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\n",
        "                 device=None,\n",
        "                 single_rationale_threshold=20.0):\n",
        "        \"\"\"\n",
        "        Initialize BiomedCLIP confirm module.\n",
        "\n",
        "        Args:\n",
        "            single_rationale_threshold: float, threshold for accepting a single rationale\n",
        "        \"\"\"\n",
        "        self.device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "\n",
        "        # Load model, tokenizer, and preprocessing transforms\n",
        "        self.model, _, self.preprocess = create_model_and_transforms(model_name)\n",
        "        self.tokenizer = get_tokenizer(model_name)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.context_length = 256  # Max tokens for rationales\n",
        "        self.single_rationale_threshold = single_rationale_threshold\n",
        "\n",
        "    def score_rationales(self, image, rationales, template=\"This is a generated rationale for an image:\"):\n",
        "        \"\"\"\n",
        "        Compute similarity scores between image and list of rationales.\n",
        "        Returns a tensor of scores.\n",
        "        \"\"\"\n",
        "        # Preprocess image\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "            image = self.preprocess(image).unsqueeze(0)  # add batch dim\n",
        "        image = image.to(self.device)\n",
        "\n",
        "        # Tokenize rationales\n",
        "        texts = self.tokenizer([template + r for r in rationales],\n",
        "                               context_length=self.context_length).to(self.device)\n",
        "\n",
        "        # Compute embeddings\n",
        "        with torch.no_grad():\n",
        "            image_features, text_features, logit_scale = self.model(image, texts)\n",
        "            scores = (image_features @ text_features.T).squeeze(0) * logit_scale\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def confirm(self, image, rationales, threshold_method='percentile',\n",
        "                top_pct=0.3, epsilon=0.05, manual_thresh=None):\n",
        "        \"\"\"\n",
        "        Accept/reject rationales based on similarity thresholds.\n",
        "\n",
        "        threshold_method: 'percentile', 'relative_max', or 'manual'\n",
        "        manual_thresh: float, only used if threshold_method='manual'\n",
        "        \"\"\"\n",
        "        scores = self.score_rationales(image, rationales)\n",
        "\n",
        "        if threshold_method == 'percentile':\n",
        "            thresh = torch.quantile(scores, 1 - top_pct)\n",
        "        elif threshold_method == 'relative_max':\n",
        "            thresh = scores.max() - epsilon\n",
        "        elif threshold_method == 'manual':\n",
        "            if manual_thresh is None:\n",
        "                raise ValueError(\"Must provide manual_thresh when using threshold_method='manual'\")\n",
        "            thresh = manual_thresh\n",
        "        else:\n",
        "            raise ValueError(\"threshold_method must be 'percentile', 'relative_max', or 'manual'\")\n",
        "\n",
        "        accepted = [r for r, s in zip(rationales, scores) if s >= thresh]\n",
        "        rejected = [r for r, s in zip(rationales, scores) if s < thresh]\n",
        "\n",
        "        return accepted, rejected, scores, float(thresh)"
      ],
      "metadata": {
        "id": "jcwEBfQNomRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "confirm_module = BiomedCLIPConfirm()"
      ],
      "metadata": {
        "id": "3tcSDPYTomuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def med_vctp(image_path, question, examples5, gCaption, rCaption = None):\n",
        "    # Define messages\n",
        "    message1 = f\"\"\"This is the medical question at hand about a specific image: {question}. Here is a global caption describing the entire image: {gCaption}.\n",
        "    Here are all regional captions for the same image that describe a certain region summarized by the label next to it: {rCaption}\n",
        "    Your goal is to identify what specific information from the global and regional captions provided are useful for understanding and answering the question.\n",
        "    Create a summary of the specific information that is concise yet informational and relevant to the question, so that it can be answered properly later.\n",
        "    The summary can be 4 sentences maximum.\n",
        "    Make the summary informative enough so that someone without access to the image can use the summary to answer the question, but make it concise enough so that someone can understand it properly\"\"\"\n",
        "    messages1 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a medical visual reasoning assistant. Base your reasoning on provided medical image context and stay visually grounded and medically accurate.\"},\n",
        "    {\"role\": \"user\", \"content\": message1}\n",
        "    ]\n",
        "\n",
        "    # Generate concepts & information\n",
        "    responseConcepts = run_llama(messages1, max_new_tokens = 400)\n",
        "    #print('1' + responseConcepts)\n",
        "    message2 = f\"\"\"Here is a summary of the relevant concepts and information useful to answering the question at hand: {responseConcepts}.\n",
        "    Here is the question that the summary provides useful information to answer: {question}\n",
        "    The answer to this question is a single word, yes or no.\n",
        "    Provided below is a list of 5 different reasoning examples, consisting of an answer to the question and a rationale quickly explaining why the answer was chosen and is correct.\n",
        "    {examples5}\n",
        "    Use only the summary provided to generate the rationale. Do not infer or introduce any information not in the summary.\n",
        "    Use the summary intended to help create the rationale and answer the question. Separate the rationale and answer by 3 hashtags, ###.\n",
        "    The rationale should be 1-2 sentences maximum and concise, and the answer should be yes or no to answer the question.\n",
        "    Make sure the rationale is written so that someone can use it to immediately answer the question given at hand\"\"\"\n",
        "    messages2 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a medical visual reasoning assistant. Base your reasoning on provided medical image context and stay visually grounded and medically accurate.\"},\n",
        "    {\"role\": \"user\", \"content\": message2}\n",
        "    ]\n",
        "    # Generate rationale & answer (Conclusion)\n",
        "    conclusion = run_llama(messages2 , max_new_tokens = 400)\n",
        "    #print('2' + conclusion)\n",
        "    if '###' in conclusion:\n",
        "        parts = conclusion.split('###')\n",
        "        rationale = parts[0].strip()\n",
        "        answer = parts[-1].strip()  # last part should be the yes/no\n",
        "\n",
        "    else:\n",
        "      rationale, answer = conclusion, 'unknown'\n",
        "\n",
        "    refined_rationale = rationaleCheck(question, responseConcepts, image_path, rationale, examples5)\n",
        "    #print('3' + refined_rationale)\n",
        "    refined_answer = run_llama(messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a medical visual reasoning assistant. Base your reasoning on provided medical image context and stay visually grounded and medically accurate.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"\"Here is a rationale to a specific question: {refined_rationale}.\n",
        "    Here is the question: {question}.\n",
        "    Use the rationale to give a yes/no answer to the question. Output should be one word.\"\"\"}\n",
        "    ])\n",
        "    #print('4' + refined_answer)\n",
        "\n",
        "    return refined_rationale, refined_answer"
      ],
      "metadata": {
        "id": "eK_YCv_nonDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rationaleCheck(question, information, image_path, rationale, examples, maxIter=3, threshold_method='manual', manual_thresh=30):\n",
        "    openedImage = Image.open(image_path)\n",
        "\n",
        "    modRationale = rationale\n",
        "    best_rationale = modRationale\n",
        "    best_score = -1.0  # initialize as very low\n",
        "\n",
        "    for x in range(maxIter):\n",
        "\n",
        "        improvMsg = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a medical visual reasoning assistant. Base your reasoning on provided medical image context and stay visually grounded and medically accurate.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Review this rationale: {modRationale} you generated for the question: {question}.\n",
        "            If it uses irrelevant or inaccurate information, remake it based strictly on the provided information base: {information}.\n",
        "Try to remake the rationale based off the information provided, and the question mentioned earlier.\n",
        "Make sure that you strictly utilize information directly from the information base to generate a conclusion with a concise rationale.\n",
        "Again, here are the 5 different reasoning examples consisting of an answer to the question and a rationale quickly explaining why the answer was chosen and is correct:\n",
        "{examples}\n",
        "**Do not use any factual information from these examples; rely solely on the summary provided.**\n",
        "Make sure the rationales you generate for the given question at hand that use the information base provided earlier are 1-2 sentences maximum, written so that someone can use it to immediately answer the question\"\"\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        accepted, rejected, scores, thresh = confirm_module.confirm(\n",
        "            openedImage, [modRationale], threshold_method, top_pct=0.5, manual_thresh=manual_thresh\n",
        "        )\n",
        "\n",
        "        # unwrap single-value tensor into float\n",
        "        if isinstance(scores, torch.Tensor):\n",
        "            current_score = float(scores.item())\n",
        "        else:\n",
        "            current_score = float(scores)\n",
        "\n",
        "        # Track the highest scoring rationale\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            best_rationale = modRationale\n",
        "\n",
        "        if accepted:\n",
        "            return modRationale\n",
        "        else:\n",
        "            modRationale = run_llama(improvMsg)\n",
        "\n",
        "    # Fallback: if no rationale passed the threshold, return the one with highest score\n",
        "    return best_rationale"
      ],
      "metadata": {
        "id": "717_KWcDoxC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples5 = \"\"\"\n",
        "Rationale: The image shows clear bilateral pulmonary infiltrates, consistent with fluid accumulation in both lungs. This pattern strongly indicates the condition is present. ### yes\n",
        "Rationale: The enlarged heart silhouette is evident, with no signs of acute pulmonary edema. Therefore, the specific cardiac abnormality is not indicated. ### no\n",
        "Rationale: The lesion appears well-circumscribed and lacks surrounding tissue invasion, suggesting it is benign. ### no\n",
        "Rationale: There is a visible opacity in the lower lobe with associated pleural effusion, which supports the diagnosis. ### yes\n",
        "Rationale: The imaging shows normal organ contours and no abnormal densities; there are no indicators for the condition in question. ### no\"\"\""
      ],
      "metadata": {
        "id": "smugAJEEpCQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def normalize(ans):\n",
        "    return ans.strip().lower().strip(string.punctuation)"
      ],
      "metadata": {
        "id": "soJE6CfFpCau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal = normalize(\"YEs.   \")\n",
        "print(normal)"
      ],
      "metadata": {
        "id": "4vBxW_6IpCjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON files\n",
        "with open(\"close_bbox.json\") as f:\n",
        "    close_bbox = json.load(f)\n",
        "\n",
        "with open(\"raw_images.json\") as f:\n",
        "    raw_images = json.load(f)\n",
        "\n",
        "with open(\"gCaptions_1.json\") as f:\n",
        "    gCaptions = json.load(f)\n",
        "\n",
        "with open(\"rCaptions_1.json\") as f:\n",
        "    rCaptions = json.load(f)\n",
        "\n",
        "# Create dictionaries for faster lookup\n",
        "image_id_to_path = {item['image_id']: item['image_path'] for item in raw_images}\n",
        "image_id_to_gcaption = {item['image_id']: item['caption'] for item in gCaptions}\n",
        "\n",
        "# Organize regional captions by image_id\n",
        "from collections import defaultdict\n",
        "image_id_to_rcaptions = defaultdict(list)\n",
        "for item in rCaptions:\n",
        "    image_id_to_rcaptions[item['image_id']].append(item)\n",
        "\n",
        "# Loop through close_bbox\n",
        "correct = 0\n",
        "total = 0\n",
        "for entry in close_bbox:\n",
        "    image_id = entry['image_id']\n",
        "    question = entry['question']\n",
        "    answer = entry.get('answer', None)  # optional\n",
        "\n",
        "    # Get image path\n",
        "    image_path = image_id_to_path.get(image_id, None)\n",
        "\n",
        "    # Get global caption\n",
        "    gCaption = image_id_to_gcaption.get(image_id, \"\")\n",
        "\n",
        "    # Get all regional captions\n",
        "    rCaptions_list = image_id_to_rcaptions.get(image_id, [])\n",
        "\n",
        "    # Combine regional captions into one string\n",
        "    rCaption_str = \"\"\n",
        "    for rc in rCaptions_list:\n",
        "        rCaption_str += f\"Label: {rc['label']}, Caption: {rc['caption'].strip()}. \"\n",
        "\n",
        "    # Strip final whitespace\n",
        "    rCaption_str = rCaption_str.strip()\n",
        "\n",
        "    # Now you have all components for the VCTP function\n",
        "    rationaleOut, answerOut = med_vctp(image_path, question, examples5, gCaption, rCaption_str)\n",
        "    if normalize(answerOut) == answer.lower():\n",
        "        correct+=1\n",
        "    else:\n",
        "        pass\n",
        "    total+=1\n",
        "    if total%100 == 0:\n",
        "        print(f\"100 done so far, {correct} correct and {total} total questions passed\")\n",
        "print(f\"Total correct = {correct}\")\n",
        "print(f\"Total = {total}\")"
      ],
      "metadata": {
        "id": "pxXhQ1PNpHzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct)"
      ],
      "metadata": {
        "id": "td_PPyVrpLFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total)"
      ],
      "metadata": {
        "id": "MTQyfEpJpNPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def med_vctp1(image_path, question, examples5, gCaption, rCaption = None):\n",
        "    # Define messages\n",
        "    message1 = f\"\"\"This is the medical question at hand about a specific image: {question}. Here is a global caption describing the entire image: {gCaption}.\n",
        "    Here are all regional captions for the same image that describe a certain region summarized by the label next to it: {rCaption}\n",
        "    Your goal is to identify what specific information from the global and regional captions provided are useful for understanding and answering the question.\n",
        "    Create a summary of the specific information that is concise yet informational and relevant to the question, so that it can be answered properly later.\n",
        "    The summary can be 4 sentences maximum.\n",
        "    Make the summary informative enough so that someone without access to the image can use the summary to answer the question, but make it concise enough so that someone can understand it properly\"\"\"\n",
        "    messages1 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a medical visual reasoning assistant. Base your reasoning on provided medical image context and stay visually grounded and medically accurate.\"},\n",
        "    {\"role\": \"user\", \"content\": message1}\n",
        "    ]\n",
        "\n",
        "    # Generate concepts & information\n",
        "    responseConcepts = run_llama(messages1, max_new_tokens = 400)\n",
        "    #print('1' + responseConcepts)\n",
        "    message2 = f\"\"\"Here is a summary of the relevant concepts and information useful to answering the question at hand: {responseConcepts}.\n",
        "    Here is the question that the summary provides useful information to answer: {question}\n",
        "    The answer to this question is a single word, yes or no.\n",
        "    Provided below is a list of 5 different reasoning examples, consisting of an answer to the question and a rationale quickly explaining why the answer was chosen and is correct.\n",
        "    {examples5}\n",
        "    Use only the summary provided to generate the rationale. Do not infer or introduce any information not in the summary.\n",
        "    Use the summary intended to help create the rationale and answer the question. Separate the rationale and answer by 3 hashtags, ###.\n",
        "    The rationale should be 1-2 sentences maximum and concise, and the answer should be yes or no to answer the question.\n",
        "    Make sure the rationale is written so that someone can use it to immediately answer the question given at hand\"\"\"\n",
        "    messages2 = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a medical visual reasoning assistant. Base your reasoning on provided medical image context and stay visually grounded and medically accurate.\"},\n",
        "    {\"role\": \"user\", \"content\": message2}\n",
        "    ]\n",
        "    # Generate rationale & answer (Conclusion)\n",
        "    conclusion = run_llama(messages2 , max_new_tokens = 400)\n",
        "    rationale, answer = parse_vctp_output(conclusion)\n",
        "    #print('2' + conclusion)\n",
        "    return rationale, answer"
      ],
      "metadata": {
        "id": "0Y-li0gdpUVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_vctp_output(conclusion):\n",
        "    # Lowercase for normalization\n",
        "    conclusion = conclusion.lower().strip()\n",
        "\n",
        "    # Remove leading numbers like \"2\" or \"2###\"\n",
        "    conclusion = conclusion.lstrip(\"0123456789. \").replace(\"2###\", \"\").strip()\n",
        "\n",
        "    # Extract yes/no\n",
        "    if conclusion.startswith(\"yes\"):\n",
        "        answer = \"yes\"\n",
        "        rationale = conclusion[len(\"yes\"):].strip(\" \\n#\")\n",
        "    elif conclusion.startswith(\"no\"):\n",
        "        answer = \"no\"\n",
        "        rationale = conclusion[len(\"no\"):].strip(\" \\n#\")\n",
        "    else:\n",
        "        # fallback: search for 'yes' or 'no' in the first 20 chars\n",
        "        if \"yes\" in conclusion[:20]:\n",
        "            answer = \"yes\"\n",
        "        elif \"no\" in conclusion[:20]:\n",
        "            answer = \"no\"\n",
        "        else:\n",
        "            answer = \"unknown\"\n",
        "        rationale = conclusion\n",
        "    return rationale, answer"
      ],
      "metadata": {
        "id": "LrtUmW4mpWe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON files\n",
        "with open(\"close_bbox.json\") as f:\n",
        "    close_bbox = json.load(f)\n",
        "\n",
        "with open(\"raw_images.json\") as f:\n",
        "    raw_images = json.load(f)\n",
        "\n",
        "with open(\"gCaptions_1.json\") as f:\n",
        "    gCaptions = json.load(f)\n",
        "\n",
        "with open(\"rCaptions_1.json\") as f:\n",
        "    rCaptions = json.load(f)\n",
        "\n",
        "# Create dictionaries for faster lookup\n",
        "image_id_to_path = {item['image_id']: item['image_path'] for item in raw_images}\n",
        "image_id_to_gcaption = {item['image_id']: item['caption'] for item in gCaptions}\n",
        "\n",
        "# Organize regional captions by image_id\n",
        "from collections import defaultdict\n",
        "image_id_to_rcaptions = defaultdict(list)\n",
        "for item in rCaptions:\n",
        "    image_id_to_rcaptions[item['image_id']].append(item)\n",
        "\n",
        "# Loop through close_bbox\n",
        "correct = 0\n",
        "total = 0\n",
        "for entry in close_bbox:\n",
        "    image_id = entry['image_id']\n",
        "    question = entry['question']\n",
        "    answer = entry.get('answer', None)  # optional\n",
        "\n",
        "    # Get image path\n",
        "    image_path = image_id_to_path.get(image_id, None)\n",
        "\n",
        "    # Get global caption\n",
        "    gCaption = image_id_to_gcaption.get(image_id, \"\")\n",
        "\n",
        "    # Get all regional captions\n",
        "    rCaptions_list = image_id_to_rcaptions.get(image_id, [])\n",
        "\n",
        "    # Combine regional captions into one string\n",
        "    rCaption_str = \"\"\n",
        "    for rc in rCaptions_list:\n",
        "        rCaption_str += f\"Label: {rc['label']}, Caption: {rc['caption'].strip()}. \"\n",
        "\n",
        "    # Strip final whitespace\n",
        "    rCaption_str = rCaption_str.strip()\n",
        "\n",
        "    # Now you have all components for the VCTP function\n",
        "    rationaleOut, answerOut = med_vctp1(image_path, question, examples5, gCaption, rCaption_str)\n",
        "    if normalize(answerOut) == answer.lower():\n",
        "        correct+=1\n",
        "    else:\n",
        "        pass\n",
        "    total+=1\n",
        "    if total%100 == 0:\n",
        "        print(f\"100 done so far, {correct} correct and {total} total questions passed\")\n",
        "print(f\"Total correct = {correct}\")\n",
        "print(f\"Total = {total}\")"
      ],
      "metadata": {
        "id": "gzrlf855pXS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct)"
      ],
      "metadata": {
        "id": "e3gYc7uMpaCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total)"
      ],
      "metadata": {
        "id": "ZLD2chJUpbTr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}